{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from qiskit_ibm_provider import IBMProvider\n",
    "from qiskit.quantum_info import hellinger_fidelity\n",
    "from evaluation_metrics import TV\n",
    "import numpy as np\n",
    "from math import pow\n",
    "import yaml\n",
    "import logging.config\n",
    "import logging\n",
    "import pandas as pd\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from math import pow\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.styles import Border, Side\n",
    "from qiskit_aer import AerSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup logger\n",
    "with open('logging.yaml', encoding=\"utf8\") as ly:\n",
    "    loggingDict = yaml.safe_load(ly)\n",
    "logging.config.dictConfig(loggingDict)\n",
    "logger = logging.getLogger(\"driver_code_ibm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../circuits/job_params_granular_T.pkl', 'rb') as f:\n",
    "    job_params = pickle.load(f)\n",
    "\n",
    "with open('../circuits/job_params_granular_T_sampler.pkl', 'rb') as f:\n",
    "    job_params_sampler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ibm_sherbrooke', 'ibm_brisbane', 'ibm_kyoto', 'ibm_nazca'}\n"
     ]
    }
   ],
   "source": [
    "systems = {i['system'] for i in job_params}\n",
    "print(systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ibm_brisbane', 'ibm_kyoto', 'ibm_nazca', 'ibm_sherbrooke'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i['system'] for i in job_params_sampler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/ideal_results.pkl', 'rb') as f:\n",
    "    ideal_results = pickle.load(f)\n",
    "\n",
    "with open('../../ibm_API_key','r') as file:\n",
    "    token = file.readline()\n",
    "\n",
    "provider = IBMProvider(token=token, instance=\"ibm-q-ncsu/nc-state/quantum-compiler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(4, 0.1), (4, 0.2), (4, 0.3), (4, 0.4), (4, 0.5), (4, 0.6), (4, 0.7), (4, 0.8), (4, 0.9), (4, 1.0), (4, 1.5), (4, 2.0), (6, 0.1), (6, 0.2), (6, 0.3), (6, 0.4), (6, 0.5), (6, 0.6), (6, 0.7), (6, 0.8), (6, 0.9), (6, 1.0), (6, 1.5), (6, 2.0), (8, 0.1), (8, 0.2), (8, 0.3), (8, 0.4), (8, 0.5), (8, 0.6), (8, 0.7), (8, 0.8), (8, 0.9), (8, 1.0), (8, 1.5), (8, 2.0), (10, 0.1), (10, 0.2), (10, 0.3), (10, 0.4), (10, 0.5), (10, 0.6), (10, 0.7), (10, 0.8), (10, 0.9), (10, 1.0), (10, 1.5), (10, 2.0)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process each of the noise level results\n",
    "def process_results(result,layout):\n",
    "\n",
    "    def layout_rev(res):\n",
    "        n = len(layout)\n",
    "        # print(self.layout)\n",
    "        b = res\n",
    "        ret = \"\"\n",
    "        for i in range(n):\n",
    "            ret += b[-1 - layout[i]]\n",
    "        return ret\n",
    "\n",
    "\n",
    "    n_shots = sum(result.values())\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def default_value():\n",
    "        return 0\n",
    "\n",
    "    ret = defaultdict(default_value)\n",
    "\n",
    "    for key in result.keys():\n",
    "        new_key = layout_rev(key)\n",
    "        ret[new_key] += result[key] / n_shots\n",
    "\n",
    "    return ret\n",
    "\n",
    "#ZNE function\n",
    "def zne(dists,degree, number_of_points, top=65,percentage_mode=False):\n",
    "\n",
    "    if percentage_mode:\n",
    "        number_of_keys = (len(dists[0].keys()) * top) // 100\n",
    "    else:\n",
    "        number_of_keys = top\n",
    "\n",
    "    #Getting the top 10 results from the distribution\n",
    "    sorted_dist = dict(sorted(dists[0].items(), key=lambda x: x[1], reverse=True))\n",
    "    top_k = list(sorted_dist.keys())[:number_of_keys]\n",
    "    remaining_keys = list(sorted_dist.keys())[number_of_keys:]\n",
    "\n",
    "    #Since this was global folding\n",
    "    x_axis = np.array([i for i in range(1,number_of_points * 2,2)])\n",
    "\n",
    "    #Extrapolation of top 10 results\n",
    "    mitigated_top_k_dist = {}\n",
    "\n",
    "    for key in top_k:\n",
    "        mitigated_top_k_dist[key] = 0\n",
    "        y_axis = np.array([ dists[i][key] if key in dists[i] else 0 for i in range(0,number_of_points)])\n",
    "        coefficients = np.polyfit(x_axis, y_axis, degree)\n",
    "        poly = np.poly1d(coefficients)\n",
    "        mitigated_top_k_dist[key] = 0 if poly(0) < 0 else poly(0)\n",
    "\n",
    "    #Calculate the sum of the mitigated top 10 distribution\n",
    "    sum_mitigated_top_k_dist = sum(mitigated_top_k_dist.values())\n",
    "    from functools import reduce\n",
    "    sum_remaining_keys = reduce(lambda acc, key_value: acc + key_value[1] if key_value[0] in remaining_keys else acc, sorted_dist.items(), 0)\n",
    "    total_sum = sum_mitigated_top_k_dist + sum_remaining_keys\n",
    "\n",
    "    #Normalize the mitigated top 10 distribution\n",
    "    normalized_mitigated_top_k_dist = dict(map(lambda x: (x[0],x[1]/total_sum),mitigated_top_k_dist.items()))\n",
    "    normalized_mitigated_top_k_dist.update(dict(map(lambda x: (x[0],x[1]/total_sum),list(filter(lambda x: x[0] in remaining_keys,sorted_dist.items())))))\n",
    "    return normalized_mitigated_top_k_dist\n",
    "\n",
    "#Function for performing folding free ZNE\n",
    "def folding_free_zne(dist,reliability,N,top=65,percentage_mode=False):\n",
    "\n",
    "    if percentage_mode:\n",
    "        number_of_keys = (len(dist.keys()) * top) // 100\n",
    "    else:\n",
    "        number_of_keys = top\n",
    "\n",
    "    #Getting the top 10 results from the distribution\n",
    "    sorted_dist = dict(sorted(dist.items(), key=lambda x: x[1], reverse=True))\n",
    "    top_k = list(sorted_dist.keys())[:number_of_keys]\n",
    "    remaining_keys = list(sorted_dist.keys())[number_of_keys:]\n",
    "\n",
    "    x_axis = [(1-reliability),1]\n",
    "\n",
    "    #Extrapolation of top k results\n",
    "    mitigated_top_k_dist = {}\n",
    "\n",
    "    for key in top_k:\n",
    "        mitigated_top_k_dist[key] = 0\n",
    "        y_axis = [dist[key] if key in dist else 0,pow(2,-N)]\n",
    "        coefficients = np.polyfit(x_axis, y_axis, 2)\n",
    "        poly = np.poly1d(coefficients)\n",
    "        mitigated_top_k_dist[key] = 0 if poly(0) < 0 else poly(0)\n",
    "\n",
    "    #Calculate the sum of the mitigated top 10 distribution\n",
    "    sum_mitigated_top_k_dist = sum(mitigated_top_k_dist.values())\n",
    "    from functools import reduce\n",
    "    sum_remaining_keys = reduce(lambda acc, key_value: acc + key_value[1] if key_value[0] in remaining_keys else acc, sorted_dist.items(), 0)\n",
    "    total_sum = sum_mitigated_top_k_dist + sum_remaining_keys\n",
    "\n",
    "    #Normalize the mitigated top 10 distribution\n",
    "    normalized_mitigated_top_k_dist = dict(map(lambda x: (x[0],x[1]/total_sum),mitigated_top_k_dist.items()))\n",
    "    normalized_mitigated_top_k_dist.update(dict(map(lambda x: (x[0],x[1]/total_sum),list(filter(lambda x: x[0] in remaining_keys,sorted_dist.items())))))\n",
    "\n",
    "    return normalized_mitigated_top_k_dist\n",
    "\n",
    "#Function for performing reliability based ZNE\n",
    "def reliability_based_zne(dists,N,reliabilities,number_of_points,top=65,percentage_mode=False):\n",
    "\n",
    "    if percentage_mode:\n",
    "        number_of_keys = (len(dists[0].keys()) * top) // 100\n",
    "    else:\n",
    "        number_of_keys = top\n",
    "\n",
    "    #Getting the top 10 results from the distribution\n",
    "    sorted_dist = dict(sorted(dists[0].items(), key=lambda x: x[1], reverse=True))\n",
    "    top_k = list(sorted_dist.keys())[:number_of_keys]\n",
    "    remaining_keys = list(sorted_dist.keys())[number_of_keys:]\n",
    "\n",
    "    x_axis = list(map(lambda x: 1-x, reliabilities))\n",
    "    x_axis = x_axis[:number_of_points] + [1]\n",
    "    #x_axis = x_axis[:number_of_points]\n",
    "\n",
    "    #Extrapolation of top 10 results\n",
    "    mitigated_top_k_dist = {}\n",
    "\n",
    "    for key in top_k:\n",
    "        mitigated_top_k_dist[key] = 0\n",
    "        y_axis = np.array([dists[i][key] if key in dists[i] else 0 for i in range(0,number_of_points)] + [pow(2,-N)]) \n",
    "        #y_axis = np.array([dists[i][key] if key in dists[i] else 0 for i in range(0,number_of_points)])\n",
    "        coefficients = np.polyfit(x_axis, y_axis, 2)\n",
    "        poly = np.poly1d(coefficients)\n",
    "        mitigated_top_k_dist[key] = 0 if poly(0) < 0 else poly(0)\n",
    "\n",
    "    #Calculate the sum of the mitigated top 10 distribution\n",
    "    sum_mitigated_top_k_dist = sum(mitigated_top_k_dist.values())\n",
    "    from functools import reduce\n",
    "    sum_remaining_keys = reduce(lambda acc, key_value: acc + key_value[1] if key_value[0] in remaining_keys else acc, sorted_dist.items(), 0)\n",
    "    total_sum = sum_mitigated_top_k_dist + sum_remaining_keys\n",
    "\n",
    "    #Normalize the mitigated top 10 distribution\n",
    "    normalized_mitigated_top_10_dist = dict(map(lambda x: (x[0],x[1]/total_sum),mitigated_top_k_dist.items()))\n",
    "    normalized_mitigated_top_10_dist.update(dict(map(lambda x: (x[0],x[1]/total_sum),list(filter(lambda x: x[0] in remaining_keys,sorted_dist.items())))))\n",
    "\n",
    "    return normalized_mitigated_top_10_dist\n",
    "\n",
    "def simulate(circ, backend, with_noise):\n",
    "    if with_noise:\n",
    "        # load noise model\n",
    "        with open(f'noise_models/{backend.name}_noise_model.pkl', 'rb') as f:\n",
    "            noise_model = pickle.load(f)\n",
    "\n",
    "        # create a simulator for backend with noise model\n",
    "        basis_gates = noise_model.basis_gates\n",
    "        coupling_map = backend.configuration().coupling_map\n",
    "\n",
    "        sim = AerSimulator(noise_model=noise_model,\n",
    "                        basis_gates=basis_gates,\n",
    "                        coupling_map=coupling_map)\n",
    "    else:\n",
    "        # create a simulator for backend\n",
    "        sim = AerSimulator()\n",
    "\n",
    "    # simulate and extract results\n",
    "    shots = 4096\n",
    "    simulator_result = sim.run(circ, shots = shots, seed_simulator=12345).result()\n",
    "    simulator_counts = simulator_result.get_counts()\n",
    "\n",
    "    #Reverse the key strings\n",
    "    simulator_counts = {k[::-1]: v/shots for k, v in simulator_counts.items()}\n",
    "\n",
    "    return simulator_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for job_param in job_params:\n",
    "    \n",
    "    logger.info(\"Processing for N = {}, T = {}, system = {}\".format(job_param['N'],job_param['T'],job_param['system']))\n",
    "    result = {'N': job_param['N'], 'T': job_param['T'], 'system': job_param['system']}\n",
    "\n",
    "    #Getting the layout\n",
    "    circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "    with open(circuit_filename, 'rb') as f:\n",
    "        circuit = pickle.load(f)\n",
    "    layout = circuit['layout']\n",
    "\n",
    "    logger.info(\"Layout: {}\".format(layout))\n",
    "    result['layout'] = layout\n",
    "\n",
    "    #Get the raw results\n",
    "    raw_results = provider.retrieve_job(job_param['job_id']).result().get_counts()\n",
    "    processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "    ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "\n",
    "    hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "    TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Initial Results:\")\n",
    "    logger.info(\"Hellinger Fidelities: {} Max: {}\".format(hellinger_fidelities, max(hellinger_fidelities)))\n",
    "    logger.info(\"TVs: {} Min: {}\".format(TVs, min(TVs)))\n",
    "    logger.info(\"\")\n",
    "\n",
    "    result['initial'] = {}\n",
    "    result['initial']['hellinger_fidelities'] = hellinger_fidelities\n",
    "    result['initial']['max_hellinger_fidelity'] = max(hellinger_fidelities)\n",
    "    result['initial']['TVs'] = TVs\n",
    "    result['initial']['min_TV'] = min(TVs)\n",
    "\n",
    "    #Get the ZNE results\n",
    "    logger.info(\"Getting ZNE results\")\n",
    "    result['zne'] = {'hellinger_fidelity': {}, 'TV': {}}\n",
    "\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_degree = 0\n",
    "    max_number_of_points = 0\n",
    "\n",
    "    min_tv = 1\n",
    "    min_degree = 0\n",
    "    min_number_of_points = 0\n",
    "\n",
    "    for degree in range(1,2):\n",
    "        for number_of_points in range(2,6):\n",
    "            zne_processed_results = zne(processed_results,degree,number_of_points)\n",
    "            result['zne']['hellinger_fidelity'][(degree,number_of_points)] = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "            result['zne']['TV'][(degree,number_of_points)] = TV(zne_processed_results,ideal_result)\n",
    "\n",
    "            if result['zne']['hellinger_fidelity'][(degree,number_of_points)] > max_hellinger_fidelity:\n",
    "                max_hellinger_fidelity = result['zne']['hellinger_fidelity'][(degree,number_of_points)]\n",
    "                max_degree = degree\n",
    "                max_number_of_points = number_of_points\n",
    "\n",
    "            if result['zne']['TV'][(degree,number_of_points)] < min_tv:\n",
    "                min_tv = result['zne']['TV'][(degree,number_of_points)]\n",
    "                min_degree = degree\n",
    "                min_number_of_points = number_of_points\n",
    "\n",
    "            logger.info(\"Degree: {}, Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(degree,number_of_points,result['zne']['hellinger_fidelity'][(degree,number_of_points)],result['zne']['TV'][(degree,number_of_points)]))\n",
    "    \n",
    "    logger.info(\"\")\n",
    "    logger.info(\"ZNE completed:\")\n",
    "    result['zne']['hellinger_fidelity']['max_hellinger_fidelity'] = max_hellinger_fidelity\n",
    "    result['zne']['hellinger_fidelity']['max_degree'] = max_degree\n",
    "    result['zne']['hellinger_fidelity']['max_number_of_points'] = max_number_of_points\n",
    "    result['zne']['TV']['min_tv'] = min_tv\n",
    "    result['zne']['TV']['min_degree'] = min_degree\n",
    "    result['zne']['TV']['min_number_of_points'] = min_number_of_points\n",
    "    logger.info(\"Max Hellinger Fidelity: {}, Degree: {}, Number of Points: {}\".format(max_hellinger_fidelity,max_degree,max_number_of_points))\n",
    "    logger.info(\"Min TV: {}, Degree: {}, Number of Points: {}\".format(min_tv,min_degree,min_number_of_points))\n",
    "    logger.info(\"\")\n",
    "\n",
    "    #Get the folding free ZNE results\n",
    "    logger.info(\"Getting Folding Free ZNE results\")\n",
    "    result['ffzne'] = {}\n",
    "    ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'])\n",
    "    result['ffzne']['hellinger_fidelity'] = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "    result['ffzne']['TV'] = TV(ffzne_mitigated_results,ideal_result)\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"FFZNE completed:\")\n",
    "    logger.info(\"Hellinger Fidelity: {}, TV: {}\".format(result['ffzne']['hellinger_fidelity'],result['ffzne']['TV']))\n",
    "    logger.info(\"\")\n",
    "\n",
    "    #Get the reliability based ZNE results\n",
    "    logger.info(\"Getting Reliability Based ZNE results\")\n",
    "    result['rbzne'] = {'hellinger_fidelity': {}, 'TV': {}}\n",
    "\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_number_of_points = 0\n",
    "    min_tv = 1\n",
    "    min_number_of_points = 0\n",
    "    \n",
    "    for number_of_points in range(2,6):\n",
    "        rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],number_of_points)\n",
    "        result['rbzne']['hellinger_fidelity'][number_of_points] = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "        result['rbzne']['TV'][number_of_points] = TV(rbzne_mitigated_results,ideal_result)\n",
    "\n",
    "        if result['rbzne']['hellinger_fidelity'][number_of_points] > max_hellinger_fidelity:\n",
    "            max_hellinger_fidelity = result['rbzne']['hellinger_fidelity'][number_of_points]\n",
    "            max_number_of_points = number_of_points\n",
    "\n",
    "        if result['rbzne']['TV'][number_of_points] < min_tv:\n",
    "            min_tv = result['rbzne']['TV'][number_of_points]\n",
    "            min_number_of_points = number_of_points\n",
    "\n",
    "        logger.info(\"Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(number_of_points,result['rbzne']['hellinger_fidelity'][number_of_points],result['rbzne']['TV'][number_of_points]))\n",
    "    \n",
    "    logger.info(\"\")\n",
    "    logger.info(\"RBZNE completed:\")\n",
    "    result['rbzne']['hellinger_fidelity']['max_hellinger_fidelity'] = max_hellinger_fidelity\n",
    "    result['rbzne']['hellinger_fidelity']['max_number_of_points'] = max_number_of_points\n",
    "    result['rbzne']['TV']['min_tv'] = min_tv\n",
    "    result['rbzne']['TV']['min_number_of_points'] = min_number_of_points\n",
    "    logger.info(\"Max Hellinger Fidelity: {}, Number of Points: {}\".format(max_hellinger_fidelity,max_number_of_points))\n",
    "    logger.info(\"Min TV: {}, Number of Points: {}\".format(min_tv,min_number_of_points))\n",
    "    logger.info(\"=====================================================================================================\")\n",
    "    logger.info(\"\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_result = list(map(lambda x: {\"N\" : x['N'], \"T\" : x['T'], \"system\" : x['system'], \n",
    "                            \"initial_helliger_fidelity\" : x['initial']['max_hellinger_fidelity'],\n",
    "                            \"initial_TV\" : x['initial']['min_TV'],\n",
    "                            \"zne_hellinger_fidelity\" : x['zne']['hellinger_fidelity'][(1,3)],\n",
    "                            #\"zne_hel_degree\" : x['zne']['hellinger_fidelity']['max_degree'] ,\n",
    "                            #\"zne_hel_number_of_points\" : x['zne']['hellinger_fidelity']['max_number_of_points'],\n",
    "                            \"zne_TV\" : x['zne']['TV'][(1,3)], \n",
    "                            #\"zne_TV_degree\" : x['zne']['TV']['min_degree'],\n",
    "                            #\"zne_TV_number_of_points\" : x['zne']['TV']['min_number_of_points'],\n",
    "                            \"ffzne_hellinger_fidelity\" : x['ffzne']['hellinger_fidelity'],\n",
    "                            \"ffzne_TV\" : x['ffzne']['TV'], \n",
    "                            \"rbzne_hellinger_fidelity\" : x['rbzne']['hellinger_fidelity'][3], \n",
    "                            #\"rbzne_hel_number_of_points\" : x['rbzne']['hellinger_fidelity']['max_number_of_points'],\n",
    "                            \"rbzne_TV\" : x['rbzne']['TV'][3],\n",
    "                            #\"rbzne_TV_number_of_points\" : x['rbzne']['TV']['min_number_of_points']\n",
    "                            },results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(updated_result)\n",
    "df['zne_hellinger_percentage_change'] = (df['zne_hellinger_fidelity'] - df['initial_helliger_fidelity'])/df['initial_helliger_fidelity']* 100\n",
    "df['zne_TV_percentage_change'] = (df['initial_TV'] - df['zne_TV'])/df['initial_TV'] * 100\n",
    "df['ffzne_hellinger_percentage_change'] = (df['ffzne_hellinger_fidelity'] - df['initial_helliger_fidelity'])/df['initial_helliger_fidelity']* 100\n",
    "df['ffzne_TV_percentage_change'] = (df['initial_TV'] - df['ffzne_TV'])/df['initial_TV'] * 100\n",
    "df['rbzne_hellinger_percentage_change'] = (df['rbzne_hellinger_fidelity'] - df['initial_helliger_fidelity'])/df['initial_helliger_fidelity']* 100\n",
    "df['rbzne_TV_percentage_change'] = (df['initial_TV'] - df['rbzne_TV'])/df['initial_TV']* 100\n",
    "\n",
    "#Arrange the columns properly\n",
    "df = df[['N','T','system',\n",
    "         'initial_helliger_fidelity',\n",
    "         'initial_TV',\n",
    "         'zne_hellinger_fidelity',\n",
    "         'zne_hellinger_percentage_change',\n",
    "         'zne_TV',\n",
    "         'zne_TV_percentage_change',\n",
    "         'ffzne_hellinger_fidelity',\n",
    "         'ffzne_hellinger_percentage_change',\n",
    "         'ffzne_TV',\n",
    "         'ffzne_TV_percentage_change',\n",
    "         'rbzne_hellinger_fidelity',\n",
    "         'rbzne_hellinger_percentage_change',\n",
    "         'rbzne_TV',\n",
    "         'rbzne_TV_percentage_change']]\n",
    "\n",
    "#Create dfs based on the system and post in different sheets of the excel\n",
    "brisbane_df = df[df['system'] == 'ibm_brisbane']\n",
    "kyoto_df = df[df['system'] == 'ibm_kyoto']\n",
    "\n",
    "with pd.ExcelWriter(\"../results/ibm_results.xlsx\") as writer:\n",
    "    brisbane_df.to_excel(writer, sheet_name='Brisbane',index=False)\n",
    "    kyoto_df.to_excel(writer, sheet_name='Kyoto',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying the percentage of top k in zne and rbzne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting the second set of results\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for job_param in job_params:\n",
    "    \n",
    "    logger.info(\"Processing for N = {}, T = {}, system = {}\".format(job_param['N'],job_param['T'],job_param['system']))\n",
    "    result = {'N': job_param['N'], 'T': job_param['T'], 'system': job_param['system']}\n",
    "\n",
    "    #Getting the layout\n",
    "    circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "    with open(circuit_filename, 'rb') as f:\n",
    "        circuit = pickle.load(f)\n",
    "    layout = circuit['layout']\n",
    "\n",
    "    logger.info(\"Layout: {}\".format(layout))\n",
    "    result['layout'] = layout\n",
    "\n",
    "    #Get the raw results\n",
    "    raw_results = provider.retrieve_job(job_param['job_id']).result().get_counts()\n",
    "    processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "    ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "\n",
    "    hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "    TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Initial Results:\")\n",
    "    logger.info(\"Hellinger Fidelities: {} Max: {}\".format(hellinger_fidelities, max(hellinger_fidelities)))\n",
    "    logger.info(\"TVs: {} Min: {}\".format(TVs, min(TVs)))\n",
    "    logger.info(\"\")\n",
    "\n",
    "    result['initial'] = {}\n",
    "    result['initial']['hellinger_fidelities'] = hellinger_fidelities\n",
    "    result['initial']['max_hellinger_fidelity'] = max(hellinger_fidelities)\n",
    "    result['initial']['TVs'] = TVs\n",
    "    result['initial']['min_TV'] = min(TVs)\n",
    "\n",
    "    #Get the ZNE results\n",
    "    logger.info(\"Getting ZNE results\")\n",
    "    result['zne'] = {'hellinger_fidelity': {}, 'TV': {}}\n",
    "\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_top_k = 0\n",
    "\n",
    "    min_tv = 1\n",
    "    min_top_k = 0\n",
    "\n",
    "    for top_k in range(10,110,10):\n",
    "        zne_processed_results = zne(processed_results,1,3,top_k)\n",
    "        result['zne']['hellinger_fidelity'][top_k] = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "        result['zne']['TV'][top_k] = TV(zne_processed_results,ideal_result)\n",
    "\n",
    "        if result['zne']['hellinger_fidelity'][top_k] > max_hellinger_fidelity:\n",
    "            max_hellinger_fidelity = result['zne']['hellinger_fidelity'][top_k]\n",
    "            max_top_k = top_k\n",
    "\n",
    "        if result['zne']['TV'][top_k] < min_tv:\n",
    "            min_tv = result['zne']['TV'][top_k]\n",
    "            min_top_k = top_k\n",
    "\n",
    "        logger.info(\"Top K: {}, Hellinger Fidelity: {}, TV: {}\".format(top_k,result['zne']['hellinger_fidelity'][top_k],result['zne']['TV'][top_k]))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"ZNE completed:\")\n",
    "    result['zne']['hellinger_fidelity']['max_hellinger_fidelity'] = max_hellinger_fidelity\n",
    "    result['zne']['hellinger_fidelity']['max_top_k'] = max_top_k\n",
    "    result['zne']['TV']['min_tv'] = min_tv\n",
    "    result['zne']['TV']['min_top_k'] = min_top_k\n",
    "    logger.info(\"Max Hellinger Fidelity: {}, Top K: {}\".format(max_hellinger_fidelity,max_top_k))\n",
    "    logger.info(\"Min TV: {}, Top K: {}\".format(min_tv,min_top_k))\n",
    "    logger.info(\"\")\n",
    "\n",
    "    #Get the reliability based ZNE results\n",
    "    logger.info(\"Getting Reliability Based ZNE results\")\n",
    "    result['rbzne'] = {'hellinger_fidelity': {}, 'TV': {}}\n",
    "\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_top_k = 0\n",
    "    min_tv = 1\n",
    "    min_top_k = 0\n",
    "    \n",
    "    for top_k in range(10,110,10):\n",
    "        rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],3,top_k)\n",
    "        result['rbzne']['hellinger_fidelity'][top_k] = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "        result['rbzne']['TV'][top_k] = TV(rbzne_mitigated_results,ideal_result)\n",
    "\n",
    "        if result['rbzne']['hellinger_fidelity'][top_k] > max_hellinger_fidelity:\n",
    "            max_hellinger_fidelity = result['rbzne']['hellinger_fidelity'][top_k]\n",
    "            max_top_k = top_k\n",
    "\n",
    "        if result['rbzne']['TV'][top_k] < min_tv:\n",
    "            min_tv = result['rbzne']['TV'][top_k]\n",
    "            min_top_k = top_k\n",
    "\n",
    "        logger.info(\"Top K: {}, Hellinger Fidelity: {}, TV: {}\".format(top_k,result['rbzne']['hellinger_fidelity'][top_k],result['rbzne']['TV'][top_k]))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"RBZNE completed:\")\n",
    "    result['rbzne']['hellinger_fidelity']['max_hellinger_fidelity'] = max_hellinger_fidelity\n",
    "    result['rbzne']['hellinger_fidelity']['max_top_k'] = max_top_k\n",
    "    result['rbzne']['TV']['min_tv'] = min_tv\n",
    "    result['rbzne']['TV']['min_top_k'] = min_top_k\n",
    "    logger.info(\"Max Hellinger Fidelity: {}, Top K: {}\".format(max_hellinger_fidelity,max_top_k))\n",
    "    logger.info(\"Min TV: {}, Top K: {}\".format(min_tv,min_top_k))\n",
    "    logger.info(\"=====================================================================================================\")\n",
    "    logger.info(\"\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_result = list(map(lambda x: {\"N_T_system\" : (x['N'],x['T'],x['system']), \n",
    "                            \"initial_helliger_fidelity\" : x['initial']['max_hellinger_fidelity'],\n",
    "                            \"initial_TV\" : x['initial']['min_TV'],\n",
    "                            \"zne_hellinger_fidelity\" : x['zne']['hellinger_fidelity'],\n",
    "                            \"zne_TV\" : x['zne']['TV'], \n",
    "                            \"rbzne_hellinger_fidelity\" : x['rbzne']['hellinger_fidelity'], \n",
    "                            \"rbzne_TV\" : x['rbzne']['TV']\n",
    "                            },results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_hellinger = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for N in range(4,11):\n",
    "    for T in range(1,4):\n",
    "        for system in ['ibm_brisbane','ibm_kyoto']:\n",
    "            top_k_hellinger['{}_{}_{}'.format(N,T,system)] = updated_result[counter]['zne_hellinger_fidelity']\n",
    "            counter += 1\n",
    "\n",
    "top_k_tv = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for N in range(4,11):\n",
    "    for T in range(1,4):\n",
    "        for system in ['ibm_brisbane','ibm_kyoto']:\n",
    "            top_k_tv['{}_{}_{}'.format(N,T,system)] = updated_result[counter]['zne_TV']\n",
    "            counter += 1\n",
    "\n",
    "#Create dfs based on the system and post in different sheets of the excel\n",
    "with pd.ExcelWriter(\"../results/top_k_comparisons.xlsx\") as writer:\n",
    "    top_k_hellinger.to_excel(writer, sheet_name='Hellinger Fidelity',index=False)\n",
    "    top_k_tv.to_excel(writer, sheet_name='TV',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability = []\n",
    "\n",
    "for N in range(4,11):\n",
    "    for T in range(1,4):\n",
    "        for system in ['ibm_brisbane','ibm_kyoto','ibm_sherbrooke']:\n",
    "            circuit_filename = \"../circuits/modified/\" + system + \"_\" + str(N) + \"_\" + str(T) + \"_reliabilities.pkl\"\n",
    "            with open(circuit_filename, 'rb') as f:\n",
    "                circuit = pickle.load(f)\n",
    "                print(circuit)\n",
    "\n",
    "            reliability.append({\"N_T_system\" : (N,T,system), \"reliability\" : circuit['circuit_reliabilities']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_df = pd.DataFrame(reliability)\n",
    "reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write this to an excel file\n",
    "reliability_df.to_excel(\"../results/reliabilities.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering case by case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing ibm_brisbane N=4 and T=1 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from math import pow\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.styles import Border, Side\n",
    "\n",
    "kyoto_job_params = list(filter(lambda x: x['system'] == 'ibm_kyoto',job_params))\n",
    "\n",
    "summary_dict_list = []\n",
    "\n",
    "for job_param in kyoto_job_params:\n",
    "\n",
    "    circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "    with open(circuit_filename, 'rb') as f:\n",
    "        circuit = pickle.load(f)\n",
    "    layout = circuit['layout']\n",
    "\n",
    "    #Getting the ideal results\n",
    "    ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "\n",
    "    #Getting the raw results\n",
    "    raw_results = provider.retrieve_job(job_param['job_id']).result().get_counts()\n",
    "    processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "\n",
    "    keys = set(ideal_result.keys()) | set(processed_results[0].keys()) | set(processed_results[1].keys()) | set(processed_results[2].keys()) | set(processed_results[3].keys()) | set(processed_results[4].keys())\n",
    "    keys = sorted(list(keys))\n",
    "\n",
    "    #Create a df and assign keys as index and name the column as State and add the values of keys to that column\n",
    "    distribution_data_df = pd.DataFrame(keys,columns=['State'],index=keys)\n",
    "\n",
    "    #Map the results to the dataframe\n",
    "    distribution_data_df['Ideal probability'] = distribution_data_df['State'].map(ideal_result)\n",
    "    distribution_data_df['Simulation probability'] = distribution_data_df['State'].map(processed_results[0])\n",
    "    distribution_data_df['Noise factor 3'] = distribution_data_df['State'].map(processed_results[1])\n",
    "    distribution_data_df['Noise factor 5'] = distribution_data_df['State'].map(processed_results[2])\n",
    "    distribution_data_df['Noise factor 7'] = distribution_data_df['State'].map(processed_results[3])\n",
    "    distribution_data_df['Noise factor 9'] = distribution_data_df['State'].map(processed_results[4])\n",
    "\n",
    "\n",
    "    hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "    initial_hellinger_fidelity = hellinger_fidelities[0]\n",
    "\n",
    "    TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "    initial_TV = TVs[0]\n",
    "\n",
    "    #ZNE\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_number_of_points = 0\n",
    "    max_degree = 0\n",
    "    max_top = 0\n",
    "\n",
    "    min_tv = 1\n",
    "    min_number_of_points = 0\n",
    "    min_degree = 0\n",
    "    min_top = 0\n",
    "\n",
    "    for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "        for degree in [1,2]:\n",
    "            for number_of_points in [2,3,4,5]:\n",
    "                zne_processed_results = zne(processed_results,degree,number_of_points,top)\n",
    "                print(\"Top: {}, Degree: {}, Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(top,degree,number_of_points,hellinger_fidelity(zne_processed_results,ideal_result),TV(zne_processed_results,ideal_result)))\n",
    "\n",
    "                if hellinger_fidelity(zne_processed_results,ideal_result) > max_hellinger_fidelity:\n",
    "                    max_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "                    max_number_of_points, max_degree, max_top = number_of_points, degree, top\n",
    "\n",
    "                if TV(zne_processed_results,ideal_result) < min_tv:\n",
    "                    min_tv = TV(zne_processed_results,ideal_result)\n",
    "                    min_number_of_points, min_degree, min_top = number_of_points, degree, top\n",
    "\n",
    "    zne_degree = max_degree\n",
    "    zne_number_of_points = max_number_of_points\n",
    "    zne_top = max_top\n",
    "\n",
    "    print(\"ZNE completed\")\n",
    "    print(\"Max Hellinger Fidelity: {}, Number of Points: {}, Degree: {}, Top: {}\".format(max_hellinger_fidelity,max_number_of_points,max_degree,max_top))\n",
    "    print(\"Min TV: {}, Number of Points: {}, Degree: {}, Top: {}\".format(min_tv,min_number_of_points,min_degree,min_top))\n",
    "\n",
    "    zne_processed_results = zne(processed_results,zne_degree,zne_number_of_points,zne_top)\n",
    "\n",
    "    best_zne_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "    best_zne_tv = TV(zne_processed_results,ideal_result)\n",
    "\n",
    "    print(\"Hellinger fidelity: {} TV: {}\".format(hellinger_fidelity(zne_processed_results,ideal_result),TV(zne_processed_results,ideal_result)))\n",
    "\n",
    "    distribution_data_df['ZNE best distribution'] = distribution_data_df['State'].map(zne_processed_results)\n",
    "\n",
    "    #ffzne\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_top = 0\n",
    "\n",
    "    min_tv = 1\n",
    "    min_top = 0\n",
    "\n",
    "    for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "        ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'],top)\n",
    "        print(\"Top: {}, Hellinger Fidelity: {}, TV: {}\".format(top,hellinger_fidelity(ffzne_mitigated_results,ideal_result),TV(ffzne_mitigated_results,ideal_result)))\n",
    "\n",
    "        if hellinger_fidelity(ffzne_mitigated_results,ideal_result) > max_hellinger_fidelity:\n",
    "            max_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "            max_top = top\n",
    "\n",
    "        if TV(ffzne_mitigated_results,ideal_result) < min_tv:\n",
    "            min_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "            min_top = top\n",
    "\n",
    "    ffzne_top = max_top\n",
    "\n",
    "    print(\"FFZNE completed\")\n",
    "    print(\"Max Hellinger Fidelity: {}, Top: {}\".format(max_hellinger_fidelity,max_top))\n",
    "    print(\"Min TV: {}, Top: {}\".format(min_tv,min_top))\n",
    "\n",
    "    ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'],ffzne_top)\n",
    "\n",
    "    best_ffzne_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "    best_ffzne_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "\n",
    "    distribution_data_df['FFZNE best distribution'] = distribution_data_df['State'].map(ffzne_mitigated_results)\n",
    "\n",
    "    #RBZNE\n",
    "    max_hellinger_fidelity = 0\n",
    "    max_number_of_points = 0\n",
    "    max_top = 0\n",
    "\n",
    "    min_tv = 1\n",
    "    min_number_of_points = 0\n",
    "    min_top = 0\n",
    "\n",
    "    for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "            for number_of_points in [2,3,4,5]:\n",
    "                rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],number_of_points,top)\n",
    "                print(\"Top: {}, Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(top,number_of_points,hellinger_fidelity(rbzne_mitigated_results,ideal_result),TV(rbzne_mitigated_results,ideal_result)))\n",
    "\n",
    "                if hellinger_fidelity(rbzne_mitigated_results,ideal_result) > max_hellinger_fidelity:\n",
    "                    max_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "                    max_number_of_points, max_top = number_of_points, top\n",
    "\n",
    "                if TV(rbzne_mitigated_results,ideal_result) < min_tv:\n",
    "                    min_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "                    min_number_of_points, min_top = number_of_points, top\n",
    "\n",
    "    rbzne_top = max_top\n",
    "    rbzne_number_of_points = max_number_of_points\n",
    "\n",
    "    print()\n",
    "    print(\"RBZNE completed\")\n",
    "    print(\"Max Hellinger Fidelity: {}, Number of Points: {}, Top: {}\".format(max_hellinger_fidelity,max_number_of_points,max_top))\n",
    "    print(\"Min TV: {}, Number of Points: {}, Top: {}\".format(min_tv,min_number_of_points,min_top))\n",
    "\n",
    "    rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],rbzne_number_of_points,rbzne_top)\n",
    "    print(\"Hellinger fidelity: {} TV: {}\".format(hellinger_fidelity(rbzne_mitigated_results,ideal_result),TV(rbzne_mitigated_results,ideal_result)))\n",
    "\n",
    "    best_rbzne_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "    best_rbzne_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "\n",
    "    distribution_data_df['RBZNE best distribution'] = distribution_data_df['State'].map(rbzne_mitigated_results)\n",
    "\n",
    "    #Sort based on Simulation probability\n",
    "    distribution_data_df = distribution_data_df.sort_values(by='Simulation probability',ascending=False)\n",
    "\n",
    "    counter = int(pow(2,job_param['N']) + 3)\n",
    "    sheet_name = 'N_{}_T_{}'.format(job_param['N'],job_param['T'])\n",
    "\n",
    "    summary_dict_list.append({'N': job_param['N'], \n",
    "                              'T': job_param['T'], \n",
    "                              'Initial Hellinger fidelity' : hellinger_fidelities[0],\n",
    "                              'Initial TV' : TVs[0],\n",
    "                              'ZNE Degree' : zne_degree,\n",
    "                                'ZNE Number of points' : zne_number_of_points,\n",
    "                                'ZNE Top k%' : zne_top,\n",
    "                                'ZNE Hellinger fidelity' : best_zne_hellinger_fidelity,\n",
    "                                'ZNE TV' : best_zne_tv,\n",
    "                                'FFZNE Top k%' : ffzne_top,\n",
    "                                'FFZNE Hellinger fidelity' : best_ffzne_hellinger_fidelity,\n",
    "                                'FFZNE TV' : best_ffzne_tv,\n",
    "                                'RBZNE Number of points' : rbzne_number_of_points,\n",
    "                                'RBZNE Top k%' : rbzne_top,\n",
    "                                'RBZNE Hellinger fidelity' : best_rbzne_hellinger_fidelity,\n",
    "                                'RBZNE TV' : best_rbzne_tv\n",
    "                                })\n",
    "\n",
    "    #write in append mode\n",
    "    with pd.ExcelWriter(\"../results/distribution_data.xlsx\",mode='a', engine='openpyxl') as writer:\n",
    "        distribution_data_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        \n",
    "        # Accessing the active sheet\n",
    "        active_sheet = workbook.active\n",
    "        \n",
    "        # Creating color scale rules\n",
    "        color_scale_rule = ColorScaleRule(start_type='min', start_color='FF00FF00', end_type='max', end_color='FF00FF00')\n",
    "        \n",
    "        # Applying color scale rules to different ranges\n",
    "        worksheet.conditional_formatting.add('H2:H{}'.format(int((zne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "        worksheet.conditional_formatting.add('I2:I{}'.format(int((ffzne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "        worksheet.conditional_formatting.add('J2:J{}'.format(int((rbzne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "\n",
    "        # Write to cell A19\n",
    "        worksheet['A{}'.format(str(counter))] = 'Hellinger fidelity'\n",
    "        worksheet['A{}'.format(str(counter+1))] = 'TV'\n",
    "        worksheet['C{}'.format(str(counter))] = initial_hellinger_fidelity\n",
    "        worksheet['C{}'.format(str(counter+1))] = initial_TV\n",
    "        worksheet['H{}'.format(str(counter))] = best_zne_hellinger_fidelity\n",
    "        worksheet['H{}'.format(str(counter+1))] = best_zne_tv\n",
    "        worksheet['I{}'.format(str(counter))] = best_ffzne_hellinger_fidelity\n",
    "        worksheet['I{}'.format(str(counter+1))] = best_ffzne_tv\n",
    "        worksheet['J{}'.format(str(counter))] = best_rbzne_hellinger_fidelity\n",
    "        worksheet['J{}'.format(str(counter+1))] = best_rbzne_tv\n",
    "\n",
    "        worksheet['A{}'.format(str(counter+3))] = 'Degree used for extrapolation'\n",
    "        worksheet['A{}'.format(str(counter+4))] = 'Number of points used for extrapolation'\n",
    "        worksheet['A{}'.format(str(counter+5))] = 'Top k percentage'\n",
    "        worksheet['H{}'.format(str(counter+3))] = zne_degree\n",
    "        worksheet['H{}'.format(str(counter+4))] = zne_number_of_points\n",
    "        worksheet['H{}'.format(str(counter+5))] = zne_top\n",
    "        worksheet['I{}'.format(str(counter+5))] = ffzne_top\n",
    "        worksheet['J{}'.format(str(counter+4))] = rbzne_number_of_points\n",
    "        worksheet['J{}'.format(str(counter+5))] = rbzne_top\n",
    "\n",
    "        #Create a border around A19 to J20\n",
    "        border = Border(left=Side(border_style='thin', color='FF000000'),\n",
    "                        right=Side(border_style='thin', color='FF000000'),\n",
    "                        top=Side(border_style='thin', color='FF000000'),\n",
    "                        bottom=Side(border_style='thin', color='FF000000'))\n",
    "        \n",
    "        left_thick_border = Border(left=Side(border_style='medium', color='FF000000'))\n",
    "        bottom_thick_border = Border(bottom=Side(border_style='medium', color='FF000000'))\n",
    "        right_thick_border = Border(right=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "        heading_border = Border(left=Side(border_style='medium', color='FF000000'),\n",
    "                                right=Side(border_style='medium', color='FF000000'),\n",
    "                                top=Side(border_style='medium', color='FF000000'),\n",
    "                                bottom=Side(border_style='medium', color='FF000000'))\n",
    "        \n",
    "        bottom_right_thick_border = Border(right=Side(border_style='medium', color='FF000000'),\n",
    "                                            bottom=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "        for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=1, max_col=1):\n",
    "            for cell in row:\n",
    "                cell.border = left_thick_border\n",
    "\n",
    "        for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=10, max_col=10):\n",
    "            for cell in row:\n",
    "                cell.border = right_thick_border\n",
    "\n",
    "        for row in worksheet.iter_rows(min_row=counter-2, max_row=counter-2, min_col=1, max_col=10):\n",
    "            for cell in row:\n",
    "                cell.border = bottom_thick_border\n",
    "        \n",
    "        for row in worksheet.iter_rows(min_row=counter, max_row=counter+1, min_col=1, max_col=10):\n",
    "            for cell in row:\n",
    "                cell.border = border\n",
    "\n",
    "        for row in worksheet.iter_rows(min_row=counter+3, max_row=counter+5, min_col=1, max_col=10):\n",
    "            for cell in row:\n",
    "                cell.border = border\n",
    "\n",
    "        for row in worksheet.iter_rows(min_row=1, max_row=1, min_col=1, max_col=10):\n",
    "            for cell in row:\n",
    "                cell.border = heading_border\n",
    "\n",
    "        #assign bottom right thick border to J19\n",
    "        worksheet['J{}'.format(str(counter-2))].border = bottom_right_thick_border\n",
    "\n",
    "        # Auto-adjusting column widths\n",
    "        for column_cells in worksheet.columns:\n",
    "            length = max(len(str(cell.value)) for cell in column_cells)\n",
    "            worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "        # Centering text in every cell\n",
    "        for row in worksheet.iter_rows():\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "summary_df = pd.DataFrame(summary_dict_list)\n",
    "with pd.ExcelWriter(\"../results/distribution_data.xlsx\",mode='a', engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        \n",
    "        # Accessing the active sheet\n",
    "        active_sheet = workbook.active\n",
    "\n",
    "        # Auto-adjusting column widths\n",
    "        for column_cells in worksheet.columns:\n",
    "            length = max(len(str(cell.value)) for cell in column_cells)\n",
    "            worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "        # Centering text in every cell\n",
    "        for row in worksheet.iter_rows():\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='center', vertical='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'ibm_brisbane', 'N': 4, 'T': 1, 'job_id': 'cq65fkp28rp0008y0tg0'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the job params and the layout that was used\n",
    "job_param = job_params[0]\n",
    "job_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "with open(circuit_filename, 'rb') as f:\n",
    "    circuit = pickle.load(f)\n",
    "layout = circuit['layout']\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the ideal results\n",
    "ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "\n",
    "#Getting the raw results\n",
    "raw_results = provider.retrieve_job(job_param['job_id']).result().get_counts()\n",
    "processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "\n",
    "keys = set(ideal_result.keys()) | set(processed_results[0].keys()) | set(processed_results[1].keys()) | set(processed_results[2].keys()) | set(processed_results[3].keys()) | set(processed_results[4].keys())\n",
    "keys = sorted(list(keys))\n",
    "\n",
    "#Create a df and assign keys as index and name the column as State and add the values of keys to that column\n",
    "distribution_data_df = pd.DataFrame(keys,columns=['State'],index=keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the ideal results to the dataframe\n",
    "distribution_data_df['Ideal probability'] = distribution_data_df['State'].map(ideal_result)\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the processed results\n",
    "distribution_data_df['Simulation probability'] = distribution_data_df['State'].map(processed_results[0])\n",
    "distribution_data_df['Noise factor 3'] = distribution_data_df['State'].map(processed_results[1])\n",
    "distribution_data_df['Noise factor 5'] = distribution_data_df['State'].map(processed_results[2])\n",
    "distribution_data_df['Noise factor 7'] = distribution_data_df['State'].map(processed_results[3])\n",
    "distribution_data_df['Noise factor 9'] = distribution_data_df['State'].map(processed_results[4])\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hellinger fidelity\n",
    "hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "initial_hellinger_fidelity = hellinger_fidelities[0]\n",
    "hellinger_fidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29839976004521235,\n",
       " 0.3740697882299207,\n",
       " 0.41128887343848936,\n",
       " 0.40005840468848936,\n",
       " 0.3249256123356375]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total variational distance\n",
    "TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "initial_TV = TVs[0]\n",
    "TVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZNE\n",
    "max_hellinger_fidelity = 0\n",
    "max_number_of_points = 0\n",
    "max_degree = 0\n",
    "max_top = 0\n",
    "\n",
    "min_tv = 1\n",
    "min_number_of_points = 0\n",
    "min_degree = 0\n",
    "min_top = 0\n",
    "\n",
    "for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "    for degree in [1,2]:\n",
    "        for number_of_points in [2,3,4,5]:\n",
    "            zne_processed_results = zne(processed_results,degree,number_of_points,top)\n",
    "            print(\"Top: {}, Degree: {}, Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(top,degree,number_of_points,hellinger_fidelity(zne_processed_results,ideal_result),TV(zne_processed_results,ideal_result)))\n",
    "\n",
    "            if hellinger_fidelity(zne_processed_results,ideal_result) > max_hellinger_fidelity:\n",
    "                max_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "                max_number_of_points, max_degree, max_top = number_of_points, degree, top\n",
    "\n",
    "            if TV(zne_processed_results,ideal_result) < min_tv:\n",
    "                min_tv = TV(zne_processed_results,ideal_result)\n",
    "                min_number_of_points, min_degree, min_top = number_of_points, degree, top\n",
    "\n",
    "zne_degree = max_degree\n",
    "zne_number_of_points = max_number_of_points\n",
    "zne_top = max_top\n",
    "\n",
    "print(\"ZNE completed\")\n",
    "print(\"Max Hellinger Fidelity: {}, Number of Points: {}, Degree: {}, Top: {}\".format(max_hellinger_fidelity,max_number_of_points,max_degree,max_top))\n",
    "print(\"Min TV: {}, Number of Points: {}, Degree: {}, Top: {}\".format(min_tv,min_number_of_points,min_degree,min_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellinger fidelity: 0.8783043803872781 TV: 0.2858136194730446\n"
     ]
    }
   ],
   "source": [
    "zne_processed_results = zne(processed_results,zne_degree,zne_number_of_points,zne_top)\n",
    "\n",
    "best_zne_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "best_zne_tv = TV(zne_processed_results,ideal_result)\n",
    "\n",
    "print(\"Hellinger fidelity: {} TV: {}\".format(hellinger_fidelity(zne_processed_results,ideal_result),TV(zne_processed_results,ideal_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_data_df['ZNE best distribution'] = distribution_data_df['State'].map(zne_processed_results)\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ffzne\n",
    "max_hellinger_fidelity = 0\n",
    "max_top = 0\n",
    "\n",
    "min_tv = 1\n",
    "min_top = 0\n",
    "\n",
    "for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "    ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'],top)\n",
    "    print(\"Top: {}, Hellinger Fidelity: {}, TV: {}\".format(top,hellinger_fidelity(ffzne_mitigated_results,ideal_result),TV(ffzne_mitigated_results,ideal_result)))\n",
    "\n",
    "    if hellinger_fidelity(ffzne_mitigated_results,ideal_result) > max_hellinger_fidelity:\n",
    "        max_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "        max_top = top\n",
    "\n",
    "    if TV(ffzne_mitigated_results,ideal_result) < min_tv:\n",
    "        min_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "        min_top = top\n",
    "\n",
    "ffzne_top = max_top\n",
    "\n",
    "print(\"FFZNE completed\")\n",
    "print(\"Max Hellinger Fidelity: {}, Top: {}\".format(max_hellinger_fidelity,max_top))\n",
    "print(\"Min TV: {}, Top: {}\".format(min_tv,min_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'],ffzne_top)\n",
    "\n",
    "best_ffzne_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "best_ffzne_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "\n",
    "distribution_data_df['FFZNE best distribution'] = distribution_data_df['State'].map(ffzne_mitigated_results)\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBZNE\n",
    "max_hellinger_fidelity = 0\n",
    "max_number_of_points = 0\n",
    "max_top = 0\n",
    "\n",
    "min_tv = 1\n",
    "min_number_of_points = 0\n",
    "min_top = 0\n",
    "\n",
    "for top in [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]:\n",
    "        for number_of_points in [2,3,4,5]:\n",
    "            rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],number_of_points,top)\n",
    "            print(\"Top: {}, Number of Points: {}, Hellinger Fidelity: {}, TV: {}\".format(top,number_of_points,hellinger_fidelity(rbzne_mitigated_results,ideal_result),TV(rbzne_mitigated_results,ideal_result)))\n",
    "\n",
    "            if hellinger_fidelity(rbzne_mitigated_results,ideal_result) > max_hellinger_fidelity:\n",
    "                max_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "                max_number_of_points, max_top = number_of_points, top\n",
    "\n",
    "            if TV(rbzne_mitigated_results,ideal_result) < min_tv:\n",
    "                min_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "                min_number_of_points, min_top = number_of_points, top\n",
    "\n",
    "rbzne_top = max_top\n",
    "rbzne_number_of_points = max_number_of_points\n",
    "\n",
    "print()\n",
    "print(\"RBZNE completed\")\n",
    "print(\"Max Hellinger Fidelity: {}, Number of Points: {}, Top: {}\".format(max_hellinger_fidelity,max_number_of_points,max_top))\n",
    "print(\"Min TV: {}, Number of Points: {}, Top: {}\".format(min_tv,min_number_of_points,min_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],rbzne_number_of_points,rbzne_top)\n",
    "print(\"Hellinger fidelity: {} TV: {}\".format(hellinger_fidelity(rbzne_mitigated_results,ideal_result),TV(rbzne_mitigated_results,ideal_result)))\n",
    "\n",
    "best_rbzne_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "best_rbzne_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "\n",
    "distribution_data_df['RBZNE best distribution'] = distribution_data_df['State'].map(rbzne_mitigated_results)\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort based on Simulation probability\n",
    "distribution_data_df = distribution_data_df.sort_values(by='Simulation probability',ascending=False)\n",
    "distribution_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "from math import pow\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.styles import Border, Side\n",
    "\n",
    "counter = int(pow(2,job_param['N']) + 3)\n",
    "sheet_name = 'N_{}_T_{}'.format(job_param['N'],job_param['T'])\n",
    "\n",
    "with pd.ExcelWriter(\"../results/distribution_data.xlsx\",mode='a', engine='openpyxl') as writer:\n",
    "    distribution_data_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    # Accessing the active sheet\n",
    "    active_sheet = workbook.active\n",
    "    \n",
    "    # Creating color scale rules\n",
    "    color_scale_rule = ColorScaleRule(start_type='min', start_color='FF00FF00', end_type='max', end_color='FF00FF00')\n",
    "    \n",
    "    # Applying color scale rules to different ranges\n",
    "    worksheet.conditional_formatting.add('H2:H{}'.format(int((zne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "    worksheet.conditional_formatting.add('I2:I{}'.format(int((ffzne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "    worksheet.conditional_formatting.add('J2:J{}'.format(int((rbzne_top * pow(2,job_param['N']) // 100) + 2)), color_scale_rule)\n",
    "\n",
    "    # Write to cell A19\n",
    "    worksheet['A{}'.format(str(counter))] = 'Hellinger fidelity'\n",
    "    worksheet['A{}'.format(str(counter+1))] = 'TV'\n",
    "    worksheet['C{}'.format(str(counter))] = initial_hellinger_fidelity\n",
    "    worksheet['C{}'.format(str(counter+1))] = initial_TV\n",
    "    worksheet['H{}'.format(str(counter))] = best_zne_hellinger_fidelity\n",
    "    worksheet['H{}'.format(str(counter+1))] = best_zne_tv\n",
    "    worksheet['I{}'.format(str(counter))] = best_ffzne_hellinger_fidelity\n",
    "    worksheet['I{}'.format(str(counter+1))] = best_ffzne_tv\n",
    "    worksheet['J{}'.format(str(counter))] = best_rbzne_hellinger_fidelity\n",
    "    worksheet['J{}'.format(str(counter+1))] = best_rbzne_tv\n",
    "\n",
    "    worksheet['A{}'.format(str(counter+3))] = 'Degree used for extrapolation'\n",
    "    worksheet['A{}'.format(str(counter+4))] = 'Number of points used for extrapolation'\n",
    "    worksheet['H{}'.format(str(counter+3))] = zne_degree\n",
    "    worksheet['H{}'.format(str(counter+4))] = zne_number_of_points\n",
    "    worksheet['J{}'.format(str(counter+4))] = rbzne_number_of_points\n",
    "\n",
    "    #Create a border around A19 to J20\n",
    "    border = Border(left=Side(border_style='thin', color='FF000000'),\n",
    "                    right=Side(border_style='thin', color='FF000000'),\n",
    "                    top=Side(border_style='thin', color='FF000000'),\n",
    "                    bottom=Side(border_style='thin', color='FF000000'))\n",
    "    \n",
    "    left_thick_border = Border(left=Side(border_style='medium', color='FF000000'))\n",
    "    bottom_thick_border = Border(bottom=Side(border_style='medium', color='FF000000'))\n",
    "    right_thick_border = Border(right=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "    heading_border = Border(left=Side(border_style='medium', color='FF000000'),\n",
    "                            right=Side(border_style='medium', color='FF000000'),\n",
    "                            top=Side(border_style='medium', color='FF000000'),\n",
    "                            bottom=Side(border_style='medium', color='FF000000'))\n",
    "    \n",
    "    bottom_right_thick_border = Border(right=Side(border_style='medium', color='FF000000'),\n",
    "                                        bottom=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "    for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=1, max_col=1):\n",
    "        for cell in row:\n",
    "            cell.border = left_thick_border\n",
    "\n",
    "    for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=10, max_col=10):\n",
    "        for cell in row:\n",
    "            cell.border = right_thick_border\n",
    "\n",
    "    for row in worksheet.iter_rows(min_row=counter-2, max_row=counter-2, min_col=1, max_col=10):\n",
    "        for cell in row:\n",
    "            cell.border = bottom_thick_border\n",
    "    \n",
    "    for row in worksheet.iter_rows(min_row=counter, max_row=counter+1, min_col=1, max_col=10):\n",
    "        for cell in row:\n",
    "            cell.border = border\n",
    "\n",
    "    for row in worksheet.iter_rows(min_row=counter+3, max_row=counter+4, min_col=1, max_col=10):\n",
    "        for cell in row:\n",
    "            cell.border = border\n",
    "\n",
    "    for row in worksheet.iter_rows(min_row=1, max_row=1, min_col=1, max_col=10):\n",
    "        for cell in row:\n",
    "            cell.border = heading_border\n",
    "\n",
    "    #assign bottom right thick border to J19\n",
    "    worksheet['J{}'.format(str(counter-2))].border = bottom_right_thick_border\n",
    "\n",
    "    # Auto-adjusting column widths\n",
    "    for column_cells in worksheet.columns:\n",
    "        length = max(len(str(cell.value)) for cell in column_cells)\n",
    "        worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "    # Centering text in every cell\n",
    "    for row in worksheet.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(horizontal='center', vertical='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the results for Top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on qiskit backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing system: ibm_sherbrooke\n",
      "Number of jobs: 48\n",
      "Processing N: 4, T: 0.1\n",
      "Processing N: 4, T: 0.2\n",
      "Processing N: 4, T: 0.3\n",
      "Processing N: 4, T: 0.4\n",
      "Processing N: 4, T: 0.5\n",
      "Processing N: 4, T: 0.6\n",
      "Processing N: 4, T: 0.7\n",
      "Processing N: 4, T: 0.8\n",
      "Processing N: 4, T: 0.9\n",
      "Processing N: 4, T: 1.0\n",
      "Processing N: 4, T: 1.5\n",
      "Processing N: 4, T: 2.0\n",
      "Processing N: 6, T: 0.1\n",
      "Processing N: 6, T: 0.2\n",
      "Processing N: 6, T: 0.3\n",
      "Processing N: 6, T: 0.4\n",
      "Processing N: 6, T: 0.5\n",
      "Processing N: 6, T: 0.6\n",
      "Processing N: 6, T: 0.7\n",
      "Processing N: 6, T: 0.8\n",
      "Processing N: 6, T: 0.9\n",
      "Processing N: 6, T: 1.0\n",
      "Processing N: 6, T: 1.5\n",
      "Processing N: 6, T: 2.0\n",
      "Processing N: 8, T: 0.1\n",
      "Processing N: 8, T: 0.2\n",
      "Processing N: 8, T: 0.3\n",
      "Processing N: 8, T: 0.4\n",
      "Processing N: 8, T: 0.5\n",
      "Processing N: 8, T: 0.6\n",
      "Processing N: 8, T: 0.7\n",
      "Processing N: 8, T: 0.8\n",
      "Processing N: 8, T: 0.9\n",
      "Processing N: 8, T: 1.0\n",
      "Processing N: 8, T: 1.5\n",
      "Processing N: 8, T: 2.0\n",
      "Processing N: 10, T: 0.1\n",
      "Processing N: 10, T: 0.2\n",
      "Processing N: 10, T: 0.3\n",
      "Processing N: 10, T: 0.4\n",
      "Processing N: 10, T: 0.5\n",
      "Processing N: 10, T: 0.6\n",
      "Processing N: 10, T: 0.7\n",
      "Processing N: 10, T: 0.8\n",
      "Processing N: 10, T: 0.9\n",
      "Processing N: 10, T: 1.0\n",
      "Processing N: 10, T: 1.5\n",
      "Processing N: 10, T: 2.0\n",
      "Processing system: ibm_brisbane\n",
      "Number of jobs: 48\n",
      "Processing N: 4, T: 0.1\n",
      "Processing N: 4, T: 0.2\n",
      "Processing N: 4, T: 0.3\n",
      "Processing N: 4, T: 0.4\n",
      "Processing N: 4, T: 0.5\n",
      "Processing N: 4, T: 0.6\n",
      "Processing N: 4, T: 0.7\n",
      "Processing N: 4, T: 0.8\n",
      "Processing N: 4, T: 0.9\n",
      "Processing N: 4, T: 1.0\n",
      "Processing N: 4, T: 1.5\n",
      "Processing N: 4, T: 2.0\n",
      "Processing N: 6, T: 0.1\n",
      "Processing N: 6, T: 0.2\n",
      "Processing N: 6, T: 0.3\n",
      "Processing N: 6, T: 0.4\n",
      "Processing N: 6, T: 0.5\n",
      "Processing N: 6, T: 0.6\n",
      "Processing N: 6, T: 0.7\n",
      "Processing N: 6, T: 0.8\n",
      "Processing N: 6, T: 0.9\n",
      "Processing N: 6, T: 1.0\n",
      "Processing N: 6, T: 1.5\n",
      "Processing N: 6, T: 2.0\n",
      "Processing N: 8, T: 0.1\n",
      "Processing N: 8, T: 0.2\n",
      "Processing N: 8, T: 0.3\n",
      "Processing N: 8, T: 0.4\n",
      "Processing N: 8, T: 0.5\n",
      "Processing N: 8, T: 0.6\n",
      "Processing N: 8, T: 0.7\n",
      "Processing N: 8, T: 0.8\n",
      "Processing N: 8, T: 0.9\n",
      "Processing N: 8, T: 1.0\n",
      "Processing N: 8, T: 1.5\n",
      "Processing N: 8, T: 2.0\n",
      "Processing N: 10, T: 0.1\n",
      "Processing N: 10, T: 0.2\n",
      "Processing N: 10, T: 0.3\n",
      "Processing N: 10, T: 0.4\n",
      "Processing N: 10, T: 0.5\n",
      "Processing N: 10, T: 0.6\n",
      "Processing N: 10, T: 0.7\n",
      "Processing N: 10, T: 0.8\n",
      "Processing N: 10, T: 0.9\n",
      "Processing N: 10, T: 1.0\n",
      "Processing N: 10, T: 1.5\n",
      "Processing N: 10, T: 2.0\n",
      "Processing system: ibm_kyoto\n",
      "Number of jobs: 48\n",
      "Processing N: 4, T: 0.1\n",
      "Processing N: 4, T: 0.2\n",
      "Processing N: 4, T: 0.3\n",
      "Processing N: 4, T: 0.4\n",
      "Processing N: 4, T: 0.5\n",
      "Processing N: 4, T: 0.6\n",
      "Processing N: 4, T: 0.7\n",
      "Processing N: 4, T: 0.8\n",
      "Processing N: 4, T: 0.9\n",
      "Processing N: 4, T: 1.0\n",
      "Processing N: 4, T: 1.5\n",
      "Processing N: 4, T: 2.0\n",
      "Processing N: 6, T: 0.1\n",
      "Processing N: 6, T: 0.2\n",
      "Processing N: 6, T: 0.3\n",
      "Processing N: 6, T: 0.4\n",
      "Processing N: 6, T: 0.5\n",
      "Processing N: 6, T: 0.6\n",
      "Processing N: 6, T: 0.7\n",
      "Processing N: 6, T: 0.8\n",
      "Processing N: 6, T: 0.9\n",
      "Processing N: 6, T: 1.0\n",
      "Processing N: 6, T: 1.5\n",
      "Processing N: 6, T: 2.0\n",
      "Processing N: 8, T: 0.1\n",
      "Processing N: 8, T: 0.2\n",
      "Processing N: 8, T: 0.3\n",
      "Processing N: 8, T: 0.4\n",
      "Processing N: 8, T: 0.5\n",
      "Processing N: 8, T: 0.6\n",
      "Processing N: 8, T: 0.7\n",
      "Processing N: 8, T: 0.8\n",
      "Processing N: 8, T: 0.9\n",
      "Processing N: 8, T: 1.0\n",
      "Processing N: 8, T: 1.5\n",
      "Processing N: 8, T: 2.0\n",
      "Processing N: 10, T: 0.1\n",
      "Processing N: 10, T: 0.2\n",
      "Processing N: 10, T: 0.3\n",
      "Processing N: 10, T: 0.4\n",
      "Processing N: 10, T: 0.5\n",
      "Processing N: 10, T: 0.6\n",
      "Processing N: 10, T: 0.7\n",
      "Processing N: 10, T: 0.8\n",
      "Processing N: 10, T: 0.9\n",
      "Processing N: 10, T: 1.0\n",
      "Processing N: 10, T: 1.5\n",
      "Processing N: 10, T: 2.0\n",
      "Processing system: ibm_nazca\n",
      "Number of jobs: 48\n",
      "Processing N: 4, T: 0.1\n",
      "Processing N: 4, T: 0.2\n",
      "Processing N: 4, T: 0.3\n",
      "Processing N: 4, T: 0.4\n",
      "Processing N: 4, T: 0.5\n",
      "Processing N: 4, T: 0.6\n",
      "Processing N: 4, T: 0.7\n",
      "Processing N: 4, T: 0.8\n",
      "Processing N: 4, T: 0.9\n",
      "Processing N: 4, T: 1.0\n",
      "Processing N: 4, T: 1.5\n",
      "Processing N: 4, T: 2.0\n",
      "Processing N: 6, T: 0.1\n",
      "Processing N: 6, T: 0.2\n",
      "Processing N: 6, T: 0.3\n",
      "Processing N: 6, T: 0.4\n",
      "Processing N: 6, T: 0.5\n",
      "Processing N: 6, T: 0.6\n",
      "Processing N: 6, T: 0.7\n",
      "Processing N: 6, T: 0.8\n",
      "Processing N: 6, T: 0.9\n",
      "Processing N: 6, T: 1.0\n",
      "Processing N: 6, T: 1.5\n",
      "Processing N: 6, T: 2.0\n",
      "Processing N: 8, T: 0.1\n",
      "Processing N: 8, T: 0.2\n",
      "Processing N: 8, T: 0.3\n",
      "Processing N: 8, T: 0.4\n",
      "Processing N: 8, T: 0.5\n",
      "Processing N: 8, T: 0.6\n",
      "Processing N: 8, T: 0.7\n",
      "Processing N: 8, T: 0.8\n",
      "Processing N: 8, T: 0.9\n",
      "Processing N: 8, T: 1.0\n",
      "Processing N: 8, T: 1.5\n",
      "Processing N: 8, T: 2.0\n",
      "Processing N: 10, T: 0.1\n",
      "Processing N: 10, T: 0.2\n",
      "Processing N: 10, T: 0.3\n",
      "Processing N: 10, T: 0.4\n",
      "Processing N: 10, T: 0.5\n",
      "Processing N: 10, T: 0.6\n",
      "Processing N: 10, T: 0.7\n",
      "Processing N: 10, T: 0.8\n",
      "Processing N: 10, T: 0.9\n",
      "Processing N: 10, T: 1.0\n",
      "Processing N: 10, T: 1.5\n",
      "Processing N: 10, T: 2.0\n"
     ]
    }
   ],
   "source": [
    "summary_dict_list = []\n",
    "\n",
    "for system in systems:\n",
    "    \n",
    "    distribution_data_df_list = []\n",
    "\n",
    "    print(\"Processing system: {}\".format(system))\n",
    "    backend = provider.get_backend(system)\n",
    "\n",
    "    system_job_params = list(filter(lambda x: x['system'] == system,job_params))\n",
    "    print(\"Number of jobs: {}\".format(len(system_job_params)))\n",
    "    \n",
    "    for job_param in system_job_params:\n",
    "        print(\"Processing N: {}, T: {}\".format(job_param['N'],job_param['T']))\n",
    "        circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "        with open(circuit_filename, 'rb') as f:\n",
    "            circuit = pickle.load(f)\n",
    "\n",
    "        reliabilities = circuit['circuit_reliabilities']\n",
    "        layout = circuit['layout']\n",
    "\n",
    "        circuit_reliability_filename = \"../circuits/modified/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \"_reliabilities.pkl\"\n",
    "        with open(circuit_reliability_filename, 'rb') as f:\n",
    "            circuit_reliabilities = pickle.load(f)['circuit_reliabilities']\n",
    "\n",
    "        #Getting the ideal results\n",
    "        ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "        #without_noise_simulation_result = simulate(circuit['circuit'][0], backend, with_noise=False)\n",
    "\n",
    "        #Getting the raw results\n",
    "        raw_results = provider.retrieve_job(job_param['job_id']).result().get_counts()\n",
    "        processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "\n",
    "        keys = set(ideal_result.keys()) | \\\n",
    "                set(processed_results[0].keys()) | \\\n",
    "                set(processed_results[1].keys()) | \\\n",
    "                set(processed_results[2].keys()) | \\\n",
    "                set(processed_results[3].keys()) | \\\n",
    "                set(processed_results[4].keys())\n",
    "        \n",
    "        keys = sorted(list(keys))\n",
    "\n",
    "        #Create a df and assign keys as index and name the column as State and add the values of keys to that column\n",
    "        distribution_data_df = pd.DataFrame(keys,columns=['State'],index=keys)\n",
    "\n",
    "        #Map the results to the dataframe\n",
    "        distribution_data_df['Ideal probability'] = distribution_data_df['State'].map(ideal_result)\n",
    "        #distribution_data_df['Without Noise Simulation probability'] = distribution_data_df['State'].map(without_noise_simulation_result)\n",
    "        distribution_data_df['Noise factor 1 probability'] = distribution_data_df['State'].map(processed_results[0])\n",
    "        distribution_data_df['Noise factor 3 probability'] = distribution_data_df['State'].map(processed_results[1])\n",
    "        distribution_data_df['Noise factor 5 probability'] = distribution_data_df['State'].map(processed_results[2])\n",
    "        distribution_data_df['Noise factor 7 probability'] = distribution_data_df['State'].map(processed_results[3])\n",
    "        distribution_data_df['Noise factor 9 probability'] = distribution_data_df['State'].map(processed_results[4])\n",
    "\n",
    "        hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "        #without_noise_hellinger_fidelity = hellinger_fidelity(ideal_result,without_noise_simulation_result)\n",
    "        initial_hellinger_fidelity = hellinger_fidelities[0]\n",
    "\n",
    "        #without_noise_TV = TV(ideal_result,without_noise_simulation_result)\n",
    "        TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "        initial_TV = TVs[0]\n",
    "\n",
    "        #ZNE\n",
    "        zne_processed_results = zne(processed_results,2,3,10)\n",
    "\n",
    "        best_zne_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "        best_zne_tv = TV(zne_processed_results,ideal_result)\n",
    "        distribution_data_df['ZNE probability distribution'] = distribution_data_df['State'].map(zne_processed_results)\n",
    "\n",
    "        #ffzne\n",
    "        ffzne_mitigated_results = folding_free_zne(processed_results[0],reliabilities[0],job_param['N'],10)\n",
    "        best_ffzne_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "        best_ffzne_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "        distribution_data_df['FFZNE probability distribution'] = distribution_data_df['State'].map(ffzne_mitigated_results)\n",
    "\n",
    "        #RBZNE\n",
    "        rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],reliabilities,2,10)\n",
    "        best_rbzne_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "        best_rbzne_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "        distribution_data_df['RBZNE probability distribution'] = distribution_data_df['State'].map(rbzne_mitigated_results)\n",
    "\n",
    "        #Sort based on Simulation probability\n",
    "        distribution_data_df = distribution_data_df.sort_values(by='Noise factor 1 probability',ascending=False)\n",
    "\n",
    "        counter = int(pow(2,job_param['N']) + 3)\n",
    "        sheet_name = 'N_{}_T_{}'.format(job_param['N'],job_param['T'])\n",
    "\n",
    "        summary_dict_list.append({  'N': job_param['N'], \n",
    "                                    'T': job_param['T'], \n",
    "                                    'System': job_param['system'],\n",
    "                                    #'Without Noise HF' : without_noise_hellinger_fidelity,\n",
    "                                    'Noise HF' : hellinger_fidelities[0],\n",
    "                                    'ZNE HF' : best_zne_hellinger_fidelity,\n",
    "                                    'FFZNE HF' : best_ffzne_hellinger_fidelity,\n",
    "                                    'RBZNE HF' : best_rbzne_hellinger_fidelity,\n",
    "                                    #'Without Noise TV' : without_noise_TV,\n",
    "                                    'Noise TV' : TVs[0],\n",
    "                                    'ZNE TV' : best_zne_tv,\n",
    "                                    'FFZNE TV' : best_ffzne_tv,\n",
    "                                    'RBZNE TV' : best_rbzne_tv\n",
    "                                })\n",
    "\n",
    "        #Save the distribution_data_df to pickle\n",
    "        distribution_data_df_list.append(distribution_data_df)\n",
    "\n",
    "        #write in append mode\n",
    "        with pd.ExcelWriter(\"../results/{}_distribution_data.xlsx\".format(system), mode='a', engine='openpyxl') as writer:\n",
    "            distribution_data_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            \n",
    "            # Accessing the active sheet\n",
    "            active_sheet = workbook.active\n",
    "            \n",
    "            # Write to cell A19\n",
    "            worksheet['A{}'.format(str(counter))] = 'Hellinger fidelity'\n",
    "            worksheet['A{}'.format(str(counter+1))] = 'TV'\n",
    "            worksheet['C{}'.format(str(counter))] = initial_hellinger_fidelity\n",
    "            worksheet['C{}'.format(str(counter+1))] = initial_TV\n",
    "            worksheet['H{}'.format(str(counter))] = best_zne_hellinger_fidelity\n",
    "            worksheet['H{}'.format(str(counter+1))] = best_zne_tv\n",
    "            worksheet['I{}'.format(str(counter))] = best_ffzne_hellinger_fidelity\n",
    "            worksheet['I{}'.format(str(counter+1))] = best_ffzne_tv\n",
    "            worksheet['J{}'.format(str(counter))] = best_rbzne_hellinger_fidelity\n",
    "            worksheet['J{}'.format(str(counter+1))] = best_rbzne_tv\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+2))] = 'Circuit reliabilities'\n",
    "            worksheet['C{}'.format(str(counter+2))] = circuit_reliabilities[0]\n",
    "            worksheet['D{}'.format(str(counter+2))] = circuit_reliabilities[1]\n",
    "            worksheet['E{}'.format(str(counter+2))] = circuit_reliabilities[2]\n",
    "            worksheet['F{}'.format(str(counter+2))] = circuit_reliabilities[3]\n",
    "            worksheet['G{}'.format(str(counter+2))] = circuit_reliabilities[4]\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+3))] = 'Circuit depth'\n",
    "            worksheet['C{}'.format(str(counter+3))] = circuit['circuit'][0].depth()\n",
    "            worksheet['D{}'.format(str(counter+3))] = circuit['circuit'][1].depth()\n",
    "            worksheet['E{}'.format(str(counter+3))] = circuit['circuit'][2].depth()\n",
    "            worksheet['F{}'.format(str(counter+3))] = circuit['circuit'][3].depth()\n",
    "            worksheet['G{}'.format(str(counter+3))] = circuit['circuit'][4].depth()\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+4))] = 'RZX count'\n",
    "            worksheet['C{}'.format(str(counter+4))] = circuit['circuit'][0].count_ops()['rzx']\n",
    "            worksheet['D{}'.format(str(counter+4))] = circuit['circuit'][1].count_ops()['rzx']\n",
    "            worksheet['E{}'.format(str(counter+4))] = circuit['circuit'][2].count_ops()['rzx']\n",
    "            worksheet['F{}'.format(str(counter+4))] = circuit['circuit'][3].count_ops()['rzx']\n",
    "            worksheet['G{}'.format(str(counter+4))] = circuit['circuit'][4].count_ops()['rzx']\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+5))] = 'RX count'\n",
    "            worksheet['C{}'.format(str(counter+5))] = circuit['circuit'][0].count_ops()['rx']\n",
    "            worksheet['D{}'.format(str(counter+5))] = circuit['circuit'][1].count_ops()['rx']\n",
    "            worksheet['E{}'.format(str(counter+5))] = circuit['circuit'][2].count_ops()['rx']\n",
    "            worksheet['F{}'.format(str(counter+5))] = circuit['circuit'][3].count_ops()['rx']\n",
    "            worksheet['G{}'.format(str(counter+5))] = circuit['circuit'][4].count_ops()['rx']\n",
    "\n",
    "            #Create a border around A19 to J21\n",
    "            border = Border(left=Side(border_style='thin', color='FF000000'),\n",
    "                            right=Side(border_style='thin', color='FF000000'),\n",
    "                            top=Side(border_style='thin', color='FF000000'),\n",
    "                            bottom=Side(border_style='thin', color='FF000000'))\n",
    "            \n",
    "            left_thick_border = Border(left=Side(border_style='medium', color='FF000000'))\n",
    "            bottom_thick_border = Border(bottom=Side(border_style='medium', color='FF000000'))\n",
    "            right_thick_border = Border(right=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "            heading_border = Border(left=Side(border_style='medium', color='FF000000'),\n",
    "                                    right=Side(border_style='medium', color='FF000000'),\n",
    "                                    top=Side(border_style='medium', color='FF000000'),\n",
    "                                    bottom=Side(border_style='medium', color='FF000000'))\n",
    "            \n",
    "            bottom_right_thick_border = Border(right=Side(border_style='medium', color='FF000000'),\n",
    "                                                bottom=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=1, max_col=1):\n",
    "                for cell in row:\n",
    "                    cell.border = left_thick_border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=10, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = right_thick_border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=counter-2, max_row=counter-2, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = bottom_thick_border\n",
    "            \n",
    "            for row in worksheet.iter_rows(min_row=counter, max_row=counter+5, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=1, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = heading_border\n",
    "\n",
    "            #assign bottom right thick border to J19\n",
    "            worksheet['J{}'.format(str(counter-2))].border = bottom_right_thick_border\n",
    "\n",
    "            # Auto-adjusting column widths\n",
    "            for column_cells in worksheet.columns:\n",
    "                length = max(len(str(cell.value)) for cell in column_cells)\n",
    "                worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "            # Centering text in every cell\n",
    "            for row in worksheet.iter_rows():\n",
    "                for cell in row:\n",
    "                    cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "    #Pickle\n",
    "    with open(\"../results/{}_distribution_data.pkl\".format(system), 'wb') as f:\n",
    "        pickle.dump(distribution_data_df_list, f)\n",
    "\n",
    "    #break\n",
    "\n",
    "\n",
    "#Running for parameteric RBZNE with extra point\n",
    "#RBZNE for parameteric and without extra point gave good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_dict_list)\n",
    "\n",
    "summary_df['ZNE HF % change'] = (summary_df['ZNE HF'] - summary_df['Noise HF']) / summary_df['Noise HF'] * 100\n",
    "summary_df['FFZNE HF % change'] = (summary_df['FFZNE HF'] - summary_df['Noise HF']) / summary_df['Noise HF'] * 100\n",
    "summary_df['RBZNE HF % change'] = (summary_df['RBZNE HF'] - summary_df['Noise HF']) / summary_df['Noise HF'] * 100\n",
    "summary_df['ZNE TV % change'] = (summary_df['Noise TV'] - summary_df['ZNE TV']) / summary_df['Noise TV'] * 100\n",
    "summary_df['FFZNE TV % change'] = (summary_df['Noise TV'] - summary_df['FFZNE TV']) / summary_df['Noise TV'] * 100\n",
    "summary_df['RBZNE TV % change'] = (summary_df['Noise TV'] - summary_df['RBZNE TV']) / summary_df['Noise TV'] * 100\n",
    "\n",
    "summary_df = summary_df[[   'N',\n",
    "                            'T',\n",
    "                            'System',\n",
    "                            'Noise HF',\n",
    "                            'ZNE HF',\n",
    "                            'ZNE HF % change',\n",
    "                            'FFZNE HF',\n",
    "                            'FFZNE HF % change',\n",
    "                            'RBZNE HF',\n",
    "                            'RBZNE HF % change',\n",
    "                            'Noise TV',\n",
    "                            'ZNE TV',\n",
    "                            'ZNE TV % change',\n",
    "                            'FFZNE TV',\n",
    "                            'FFZNE TV % change',\n",
    "                            'RBZNE TV',\n",
    "                            'RBZNE TV % change'\n",
    "                        ]]\n",
    "\n",
    "                         \n",
    "# Pickle the summary_df\n",
    "summary_df.to_pickle(\"../results/summary_df.pkl\")\n",
    "\n",
    "sheet_name = 'Summary'\n",
    "\n",
    "with pd.ExcelWriter(\"../results/summary_data.xlsx\".format(system), engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        \n",
    "        # Accessing the active sheet\n",
    "        active_sheet = workbook.active\n",
    "\n",
    "        # Calculate Geometric mean of 1 + (% change / 100) for Hellinger fidelity and TV\n",
    "        worksheet['A195'] = 'Geometric mean'\n",
    "        worksheet['F195'] = (1 + summary_df['ZNE HF % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "        worksheet['H195'] = (1 + summary_df['FFZNE HF % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "        worksheet['J195'] = (1 + summary_df['RBZNE HF % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "        worksheet['M195'] = (1 + summary_df['ZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "        worksheet['O195'] = (1 + summary_df['FFZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "        worksheet['Q195'] = (1 + summary_df['RBZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "\n",
    "        # Auto-adjusting column widths\n",
    "        for column_cells in worksheet.columns:\n",
    "            length = max(len(str(cell.value)) for cell in column_cells)\n",
    "            worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "        # Centering text in every cell\n",
    "        for row in worksheet.iter_rows():\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='center', vertical='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing system: ibm_kyoto\n",
      "Number of jobs: 14\n",
      "Processing N: 4, T: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#Getting the raw results\u001b[39;00m\n\u001b[0;32m     28\u001b[0m quasi_dists \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mjob(job_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mquasi_dists\n\u001b[1;32m---> 29\u001b[0m quasi_dists \u001b[38;5;241m=\u001b[39m QuasiDistribution(\u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mquasi_dists[\u001b[38;5;241m0\u001b[39m],shots\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     30\u001b[0m quasi_dists\u001b[38;5;241m.\u001b[39mnearest_probability_distribution()\n\u001b[0;32m     31\u001b[0m binary_key_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mbin\u001b[39m(x)[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m127\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit.result import QuasiDistribution\n",
    "\n",
    "service = QiskitRuntimeService(channel=\"ibm_quantum\")\n",
    "\n",
    "for system in systems:\n",
    "    print(\"Processing system: {}\".format(system))\n",
    "\n",
    "    system_job_params = list(filter(lambda x: x['system'] == system,job_params_sampler))\n",
    "    print(\"Number of jobs: {}\".format(len(system_job_params)))\n",
    "    summary_dict_list = []\n",
    "\n",
    "    for job_param in system_job_params:\n",
    "        print(\"Processing N: {}, T: {}\".format(job_param['N'],job_param['T']))\n",
    "        circuit_filename = \"../circuits/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \".pkl\"\n",
    "        with open(circuit_filename, 'rb') as f:\n",
    "            circuit = pickle.load(f)\n",
    "        layout = circuit['layout']\n",
    "\n",
    "        circuit_reliability_filename = \"../circuits/modified/\" + job_param['system'] + \"_\" + str(job_param['N']) + \"_\" + str(job_param['T']) + \"_reliabilities.pkl\"\n",
    "        with open(circuit_reliability_filename, 'rb') as f:\n",
    "            circuit_reliabilities = pickle.load(f)['circuit_reliabilities']\n",
    "\n",
    "        #Getting the ideal results\n",
    "        ideal_result = ideal_results[job_param['N'],job_param['T']]\n",
    "\n",
    "        #Getting the raw results\n",
    "        quasi_dists = service.job(job_param['job_id']).result().quasi_dists\n",
    "        quasi_dists = QuasiDistribution(result.quasi_dists[0],shots=1024)\n",
    "        quasi_dists.nearest_probability_distribution()\n",
    "        binary_key_lambda = lambda x: bin(x)[2:].zfill(127)\n",
    "        binary_keys = map(binary_key_lambda, quasi_dists.keys())\n",
    "\n",
    "        raw_result = dict(zip(binary_keys, quasi_dists.values()))\n",
    "\n",
    "\n",
    "        processed_results = list(map(lambda x: process_results(x,layout),raw_results))\n",
    "\n",
    "        keys = set(ideal_result.keys()) | set(processed_results[0].keys()) | set(processed_results[1].keys()) | set(processed_results[2].keys()) | set(processed_results[3].keys()) | set(processed_results[4].keys())\n",
    "        keys = sorted(list(keys))\n",
    "\n",
    "        #Create a df and assign keys as index and name the column as State and add the values of keys to that column\n",
    "        distribution_data_df = pd.DataFrame(keys,columns=['State'],index=keys)\n",
    "\n",
    "        #Map the results to the dataframe\n",
    "        distribution_data_df['Ideal probability'] = distribution_data_df['State'].map(ideal_result)\n",
    "        distribution_data_df['Simulation probability'] = distribution_data_df['State'].map(processed_results[0])\n",
    "        distribution_data_df['Noise factor 3'] = distribution_data_df['State'].map(processed_results[1])\n",
    "        distribution_data_df['Noise factor 5'] = distribution_data_df['State'].map(processed_results[2])\n",
    "        distribution_data_df['Noise factor 7'] = distribution_data_df['State'].map(processed_results[3])\n",
    "        distribution_data_df['Noise factor 9'] = distribution_data_df['State'].map(processed_results[4])\n",
    "\n",
    "        hellinger_fidelities = list(map(lambda x: hellinger_fidelity(ideal_result,x),processed_results))\n",
    "        initial_hellinger_fidelity = hellinger_fidelities[0]\n",
    "\n",
    "        TVs = list(map(lambda x: TV(ideal_result,x),processed_results))\n",
    "        initial_TV = TVs[0]\n",
    "\n",
    "        #ZNE\n",
    "        zne_processed_results = zne(processed_results,2,3,10)\n",
    "\n",
    "        best_zne_hellinger_fidelity = hellinger_fidelity(zne_processed_results,ideal_result)\n",
    "        best_zne_tv = TV(zne_processed_results,ideal_result)\n",
    "        distribution_data_df['ZNE best distribution'] = distribution_data_df['State'].map(zne_processed_results)\n",
    "\n",
    "        #ffzne\n",
    "        ffzne_mitigated_results = folding_free_zne(processed_results[0],job_param['N'],job_param['T'],job_param['system'],10)\n",
    "        best_ffzne_hellinger_fidelity = hellinger_fidelity(ffzne_mitigated_results,ideal_result)\n",
    "        best_ffzne_tv = TV(ffzne_mitigated_results,ideal_result)\n",
    "        distribution_data_df['FFZNE best distribution'] = distribution_data_df['State'].map(ffzne_mitigated_results)\n",
    "\n",
    "        #RBZNE\n",
    "        rbzne_mitigated_results = reliability_based_zne(processed_results,job_param['N'],job_param['T'],job_param['system'],3,10)\n",
    "        best_rbzne_hellinger_fidelity = hellinger_fidelity(rbzne_mitigated_results,ideal_result)\n",
    "        best_rbzne_tv = TV(rbzne_mitigated_results,ideal_result)\n",
    "        distribution_data_df['RBZNE best distribution'] = distribution_data_df['State'].map(rbzne_mitigated_results)\n",
    "\n",
    "        #Sort based on Simulation probability\n",
    "        distribution_data_df = distribution_data_df.sort_values(by='Simulation probability',ascending=False)\n",
    "\n",
    "        counter = int(pow(2,job_param['N']) + 3)\n",
    "        sheet_name = 'N_{}_T_{}'.format(job_param['N'],job_param['T'])\n",
    "\n",
    "        summary_dict_list.append({'N': job_param['N'], \n",
    "                                'T': job_param['T'], \n",
    "                                'Initial Hellinger fidelity' : hellinger_fidelities[0],\n",
    "                                'Initial TV' : TVs[0],\n",
    "                                    'ZNE Hellinger fidelity' : best_zne_hellinger_fidelity,\n",
    "                                    'ZNE TV' : best_zne_tv,\n",
    "                                    'FFZNE Hellinger fidelity' : best_ffzne_hellinger_fidelity,\n",
    "                                    'FFZNE TV' : best_ffzne_tv,\n",
    "                                    'RBZNE Hellinger fidelity' : best_rbzne_hellinger_fidelity,\n",
    "                                    'RBZNE TV' : best_rbzne_tv\n",
    "                                    })\n",
    "\n",
    "        #write in append mode\n",
    "        with pd.ExcelWriter(\"../results/{}_distribution_data.xlsx\".format(system),mode='a', engine='openpyxl') as writer:\n",
    "            distribution_data_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            \n",
    "            # Accessing the active sheet\n",
    "            active_sheet = workbook.active\n",
    "            \n",
    "            # Write to cell A19\n",
    "            worksheet['A{}'.format(str(counter))] = 'Hellinger fidelity'\n",
    "            worksheet['A{}'.format(str(counter+1))] = 'TV'\n",
    "            worksheet['C{}'.format(str(counter))] = initial_hellinger_fidelity\n",
    "            worksheet['C{}'.format(str(counter+1))] = initial_TV\n",
    "            worksheet['H{}'.format(str(counter))] = best_zne_hellinger_fidelity\n",
    "            worksheet['H{}'.format(str(counter+1))] = best_zne_tv\n",
    "            worksheet['I{}'.format(str(counter))] = best_ffzne_hellinger_fidelity\n",
    "            worksheet['I{}'.format(str(counter+1))] = best_ffzne_tv\n",
    "            worksheet['J{}'.format(str(counter))] = best_rbzne_hellinger_fidelity\n",
    "            worksheet['J{}'.format(str(counter+1))] = best_rbzne_tv\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+2))] = 'Circuit reliabilities'\n",
    "            worksheet['C{}'.format(str(counter+2))] = circuit_reliabilities[0]\n",
    "            worksheet['D{}'.format(str(counter+2))] = circuit_reliabilities[1]\n",
    "            worksheet['E{}'.format(str(counter+2))] = circuit_reliabilities[2]\n",
    "            worksheet['F{}'.format(str(counter+2))] = circuit_reliabilities[3]\n",
    "            worksheet['G{}'.format(str(counter+2))] = circuit_reliabilities[4]\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+3))] = 'Circuit depth'\n",
    "            worksheet['C{}'.format(str(counter+3))] = circuit['circuit'][0].depth()\n",
    "            worksheet['D{}'.format(str(counter+3))] = circuit['circuit'][1].depth()\n",
    "            worksheet['E{}'.format(str(counter+3))] = circuit['circuit'][2].depth()\n",
    "            worksheet['F{}'.format(str(counter+3))] = circuit['circuit'][3].depth()\n",
    "            worksheet['G{}'.format(str(counter+3))] = circuit['circuit'][4].depth()\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+4))] = 'RZX count'\n",
    "            worksheet['C{}'.format(str(counter+4))] = circuit['circuit'][0].count_ops()['rzx']\n",
    "            worksheet['D{}'.format(str(counter+4))] = circuit['circuit'][1].count_ops()['rzx']\n",
    "            worksheet['E{}'.format(str(counter+4))] = circuit['circuit'][2].count_ops()['rzx']\n",
    "            worksheet['F{}'.format(str(counter+4))] = circuit['circuit'][3].count_ops()['rzx']\n",
    "            worksheet['G{}'.format(str(counter+4))] = circuit['circuit'][4].count_ops()['rzx']\n",
    "\n",
    "            worksheet['A{}'.format(str(counter+5))] = 'RX count'\n",
    "            worksheet['C{}'.format(str(counter+5))] = circuit['circuit'][0].count_ops()['rx']\n",
    "            worksheet['D{}'.format(str(counter+5))] = circuit['circuit'][1].count_ops()['rx']\n",
    "            worksheet['E{}'.format(str(counter+5))] = circuit['circuit'][2].count_ops()['rx']\n",
    "            worksheet['F{}'.format(str(counter+5))] = circuit['circuit'][3].count_ops()['rx']\n",
    "            worksheet['G{}'.format(str(counter+5))] = circuit['circuit'][4].count_ops()['rx']\n",
    "\n",
    "            #Create a border around A19 to J21\n",
    "            border = Border(left=Side(border_style='thin', color='FF000000'),\n",
    "                            right=Side(border_style='thin', color='FF000000'),\n",
    "                            top=Side(border_style='thin', color='FF000000'),\n",
    "                            bottom=Side(border_style='thin', color='FF000000'))\n",
    "            \n",
    "            left_thick_border = Border(left=Side(border_style='medium', color='FF000000'))\n",
    "            bottom_thick_border = Border(bottom=Side(border_style='medium', color='FF000000'))\n",
    "            right_thick_border = Border(right=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "            heading_border = Border(left=Side(border_style='medium', color='FF000000'),\n",
    "                                    right=Side(border_style='medium', color='FF000000'),\n",
    "                                    top=Side(border_style='medium', color='FF000000'),\n",
    "                                    bottom=Side(border_style='medium', color='FF000000'))\n",
    "            \n",
    "            bottom_right_thick_border = Border(right=Side(border_style='medium', color='FF000000'),\n",
    "                                                bottom=Side(border_style='medium', color='FF000000'))\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=1, max_col=1):\n",
    "                for cell in row:\n",
    "                    cell.border = left_thick_border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=counter-2, min_col=10, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = right_thick_border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=counter-2, max_row=counter-2, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = bottom_thick_border\n",
    "            \n",
    "            for row in worksheet.iter_rows(min_row=counter, max_row=counter+5, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = border\n",
    "\n",
    "            for row in worksheet.iter_rows(min_row=1, max_row=1, min_col=1, max_col=10):\n",
    "                for cell in row:\n",
    "                    cell.border = heading_border\n",
    "\n",
    "            #assign bottom right thick border to J19\n",
    "            worksheet['J{}'.format(str(counter-2))].border = bottom_right_thick_border\n",
    "\n",
    "            # Auto-adjusting column widths\n",
    "            for column_cells in worksheet.columns:\n",
    "                length = max(len(str(cell.value)) for cell in column_cells)\n",
    "                worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "            # Centering text in every cell\n",
    "            for row in worksheet.iter_rows():\n",
    "                for cell in row:\n",
    "                    cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_dict_list)\n",
    "\n",
    "    summary_df['ZNE Hellinger fidelity % change'] = (summary_df['ZNE Hellinger fidelity'] - summary_df['Initial Hellinger fidelity']) / summary_df['Initial Hellinger fidelity'] * 100\n",
    "    summary_df['ZNE TV % change'] = (summary_df['Initial TV'] - summary_df['ZNE TV']) / summary_df['Initial TV'] * 100\n",
    "    summary_df['FFZNE Hellinger fidelity % change'] = (summary_df['FFZNE Hellinger fidelity'] - summary_df['Initial Hellinger fidelity']) / summary_df['Initial Hellinger fidelity'] * 100\n",
    "    summary_df['FFZNE TV % change'] = (summary_df['Initial TV'] - summary_df['FFZNE TV']) / summary_df['Initial TV'] * 100\n",
    "    summary_df['RBZNE Hellinger fidelity % change'] = (summary_df['RBZNE Hellinger fidelity'] - summary_df['Initial Hellinger fidelity']) / summary_df['Initial Hellinger fidelity'] * 100\n",
    "    summary_df['RBZNE TV % change'] = (summary_df['Initial TV'] - summary_df['RBZNE TV']) / summary_df['Initial TV'] * 100\n",
    "\n",
    "    summary_df = summary_df[['N','T','Initial Hellinger fidelity','Initial TV',\n",
    "                            'ZNE Hellinger fidelity','ZNE Hellinger fidelity % change','ZNE TV','ZNE TV % change',\n",
    "                            'FFZNE Hellinger fidelity','FFZNE Hellinger fidelity % change','FFZNE TV','FFZNE TV % change',\n",
    "                            'RBZNE Hellinger fidelity','RBZNE Hellinger fidelity % change','RBZNE TV','RBZNE TV % change']]\n",
    "\n",
    "    sheet_name = 'Summary'\n",
    "\n",
    "    with pd.ExcelWriter(\"../results/{}_distribution_data.xlsx\".format(system),mode='a', engine='openpyxl') as writer:\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            \n",
    "            # Accessing the active sheet\n",
    "            active_sheet = workbook.active\n",
    "\n",
    "            # Calculate Geometric mean of 1 + (% change / 100) for Hellinger fidelity and TV\n",
    "            worksheet['A24'] = 'Geometric mean'\n",
    "            worksheet['F24'] = (1 + summary_df['ZNE Hellinger fidelity % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "            worksheet['H24'] = (1 + summary_df['ZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "            worksheet['J24'] = (1 + summary_df['FFZNE Hellinger fidelity % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "            worksheet['L24'] = (1 + summary_df['FFZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "            worksheet['N24'] = (1 + summary_df['RBZNE Hellinger fidelity % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "            worksheet['P24'] = (1 + summary_df['RBZNE TV % change'] / 100).prod() ** (1 / len(summary_df))\n",
    "\n",
    "            # Auto-adjusting column widths\n",
    "            for column_cells in worksheet.columns:\n",
    "                length = max(len(str(cell.value)) for cell in column_cells)\n",
    "                worksheet.column_dimensions[column_cells[0].column_letter].width = length\n",
    "\n",
    "            # Centering text in every cell\n",
    "            for row in worksheet.iter_rows():\n",
    "                for cell in row:\n",
    "                    cell.alignment = Alignment(horizontal='center', vertical='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do ZNE and RBZNE where it matters the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Simulation probability</th>\n",
       "      <th>Ideal probability</th>\n",
       "      <th>Percentage Difference in Simulation and Ideal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1111</td>\n",
       "      <td>0.197998</td>\n",
       "      <td>0.117405</td>\n",
       "      <td>68.644960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1110</td>\n",
       "      <td>0.124268</td>\n",
       "      <td>0.039047</td>\n",
       "      <td>218.253080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>0.210729</td>\n",
       "      <td>-49.834776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011</td>\n",
       "      <td>0.101318</td>\n",
       "      <td>0.099972</td>\n",
       "      <td>1.346508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1100</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>0.099972</td>\n",
       "      <td>-21.120670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>0.109919</td>\n",
       "      <td>-43.806518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0111</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>0.039047</td>\n",
       "      <td>55.687656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1001</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>-33.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010</td>\n",
       "      <td>0.041260</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>698.488562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>-15.974602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.109919</td>\n",
       "      <td>-75.345943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0100</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>400.827145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0101</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>3516.530501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0110</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>98.802386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1011</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>-25.943717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1010</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>2083.565585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State  Simulation probability  Ideal probability  \\\n",
       "15  1111                0.197998           0.117405   \n",
       "14  1110                0.124268           0.039047   \n",
       "0   0000                0.105713           0.210729   \n",
       "3   0011                0.101318           0.099972   \n",
       "12  1100                0.078857           0.099972   \n",
       "8   1000                0.061768           0.109919   \n",
       "7   0111                0.060791           0.039047   \n",
       "9   1001                0.053955           0.080880   \n",
       "2   0010                0.041260           0.005167   \n",
       "13  1101                0.028809           0.034286   \n",
       "1   0001                0.027100           0.109919   \n",
       "4   0100                0.025879           0.005167   \n",
       "5   0101                0.025879           0.000716   \n",
       "6   0110                0.025391           0.012772   \n",
       "11  1011                0.025391           0.034286   \n",
       "10  1010                0.015625           0.000716   \n",
       "\n",
       "    Percentage Difference in Simulation and Ideal  \n",
       "15                                      68.644960  \n",
       "14                                     218.253080  \n",
       "0                                      -49.834776  \n",
       "3                                        1.346508  \n",
       "12                                     -21.120670  \n",
       "8                                      -43.806518  \n",
       "7                                       55.687656  \n",
       "9                                      -33.289800  \n",
       "2                                      698.488562  \n",
       "13                                     -15.974602  \n",
       "1                                      -75.345943  \n",
       "4                                      400.827145  \n",
       "5                                     3516.530501  \n",
       "6                                       98.802386  \n",
       "11                                     -25.943717  \n",
       "10                                    2083.565585  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_data_df['Percentage Difference in Simulation and Ideal'] = (distribution_data_df['Simulation probability'] - distribution_data_df['Ideal probability'])/distribution_data_df['Ideal probability'] * 100\n",
    "distribution_data_df[['State','Simulation probability','Ideal probability','Percentage Difference in Simulation and Ideal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simuq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
